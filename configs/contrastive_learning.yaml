# === configs/contrastive_learning.yaml ===
# Contrastive Learning Transformer settings

model:
  name: 'ContrastiveTransformer'
  encoding_dim: 10
  batch_size: 32
  epochs: 50
  learning_rate: 0.0001
  num_heads: 4
  hidden_dim: 128  # Previously 'd_model', renamed for PyTorch compatibility
  num_layers: 4
  output_dim: 64  # Transformer output size
  loss_function: 'cross_entropy'  # Updated for PyTorch
  optimizer: 'adam'
  device: 'mps'  # Change to 'cpu' if no GPU is available
  seed: 42  # Random seed for reproducibility

# Model Saving & Loading
save_model: True
save_model_path: 'models/contrastive_learning.pt'
load_model: False
load_model_path: 'models/contrastive_learning.pt'

# Logging & Checkpoints
log_interval: 10
checkpoint_interval: 10
checkpoint_path: 'models/contrastive_learning_checkpoint.pt'

# Checkpoint Components
checkpoint_model: 'models/contrastive_learning_checkpoint_model.pt'
checkpoint_optimizer: 'models/contrastive_learning_checkpoint_optimizer.pt'
checkpoint_scheduler: 'models/contrastive_learning_checkpoint_scheduler.pt'
